{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84c8ea7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:12:44.847346Z",
     "iopub.status.busy": "2025-03-24T04:12:44.847076Z",
     "iopub.status.idle": "2025-03-24T04:13:14.286924Z",
     "shell.execute_reply": "2025-03-24T04:13:14.285768Z"
    },
    "id": "p-DDfRctw0vj",
    "outputId": "19f3039f-36a0-4266-cf15-e267659877a6",
    "papermill": {
     "duration": 29.451873,
     "end_time": "2025-03-24T04:13:14.288898",
     "exception": false,
     "start_time": "2025-03-24T04:12:44.837025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.5.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/site-packages (0.20.0)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/site-packages (2.5.0)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch) (10.3.5.147)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2025.2.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.5)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch) (9.1.0.70)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch) (11.6.1.9)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from torchvision) (2.0.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/site-packages (from torchvision) (11.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (3.10.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (3.2.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib) (11.1.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (4.56.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/site-packages (from matplotlib) (2.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (2.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/site-packages (11.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/site-packages (4.11.0.86)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/site-packages (from opencv-python) (2.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (1.15.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from scipy) (2.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\r\n",
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/315.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m256.0/315.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from imageio) (2.0.2)\r\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/site-packages (from imageio) (11.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: imageio\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed imageio-2.37.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.67.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install opencv-python\n",
    "!pip install scipy\n",
    "!pip install imageio\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f351bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:14.308622Z",
     "iopub.status.busy": "2025-03-24T04:13:14.308354Z",
     "iopub.status.idle": "2025-03-24T04:13:57.137040Z",
     "shell.execute_reply": "2025-03-24T04:13:57.135296Z"
    },
    "id": "dRH48ygw1Zv6",
    "papermill": {
     "duration": 42.840342,
     "end_time": "2025-03-24T04:13:57.138598",
     "exception": false,
     "start_time": "2025-03-24T04:13:14.298256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import CelebA\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "import cv2\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d565f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.158265Z",
     "iopub.status.busy": "2025-03-24T04:13:57.157844Z",
     "iopub.status.idle": "2025-03-24T04:13:57.182681Z",
     "shell.execute_reply": "2025-03-24T04:13:57.181656Z"
    },
    "id": "It0HkD_41cq8",
    "outputId": "c2ad2580-a033-4d08-fc94-16753e32150e",
    "papermill": {
     "duration": 0.036694,
     "end_time": "2025-03-24T04:13:57.184174",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.147480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b216a4f9d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "manualSeed = 999\n",
    "\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0ed783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.203962Z",
     "iopub.status.busy": "2025-03-24T04:13:57.203732Z",
     "iopub.status.idle": "2025-03-24T04:13:57.207639Z",
     "shell.execute_reply": "2025-03-24T04:13:57.206350Z"
    },
    "id": "9k8Q6LEd1h7H",
    "papermill": {
     "duration": 0.01504,
     "end_time": "2025-03-24T04:13:57.208687",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.193647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, root, transform=None):\n",
    "#         self.root = root\n",
    "#         self.transform = transform\n",
    "#         self.image_paths = sorted([f for f in os.listdir(root) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.root, self.image_paths[idx])\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9f2a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.227577Z",
     "iopub.status.busy": "2025-03-24T04:13:57.227289Z",
     "iopub.status.idle": "2025-03-24T04:13:57.230891Z",
     "shell.execute_reply": "2025-03-24T04:13:57.229860Z"
    },
    "id": "iVg9JyQV1-TP",
    "papermill": {
     "duration": 0.015155,
     "end_time": "2025-03-24T04:13:57.232416",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.217261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Transformation pipeline for high-quality images\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(512),\n",
    "#     transforms.CenterCrop(512),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363d066c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.251324Z",
     "iopub.status.busy": "2025-03-24T04:13:57.251102Z",
     "iopub.status.idle": "2025-03-24T04:13:57.254358Z",
     "shell.execute_reply": "2025-03-24T04:13:57.253464Z"
    },
    "id": "yj787i_wjvgn",
    "outputId": "46b57e46-783f-4738-8d98-ce3772beaa6a",
    "papermill": {
     "duration": 0.014883,
     "end_time": "2025-03-24T04:13:57.255992",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.241109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip flickrfaceshq-dataset-ffhq.zip -d images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea77e443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.275017Z",
     "iopub.status.busy": "2025-03-24T04:13:57.274805Z",
     "iopub.status.idle": "2025-03-24T04:13:57.278047Z",
     "shell.execute_reply": "2025-03-24T04:13:57.277030Z"
    },
    "id": "RP8xd1is2s03",
    "papermill": {
     "duration": 0.014806,
     "end_time": "2025-03-24T04:13:57.279550",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.264744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataroot = \"/content/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7047cb02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.298210Z",
     "iopub.status.busy": "2025-03-24T04:13:57.297970Z",
     "iopub.status.idle": "2025-03-24T04:13:57.301243Z",
     "shell.execute_reply": "2025-03-24T04:13:57.300379Z"
    },
    "id": "6ejSF_0N6qcB",
    "papermill": {
     "duration": 0.014884,
     "end_time": "2025-03-24T04:13:57.302893",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.288009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = ImageDataset(root=dataroot, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f558c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.321853Z",
     "iopub.status.busy": "2025-03-24T04:13:57.321614Z",
     "iopub.status.idle": "2025-03-24T04:13:57.325033Z",
     "shell.execute_reply": "2025-03-24T04:13:57.323863Z"
    },
    "id": "abA16OsY6xZE",
    "papermill": {
     "duration": 0.014866,
     "end_time": "2025-03-24T04:13:57.326534",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.311668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d0c99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.345698Z",
     "iopub.status.busy": "2025-03-24T04:13:57.345467Z",
     "iopub.status.idle": "2025-03-24T04:13:57.350031Z",
     "shell.execute_reply": "2025-03-24T04:13:57.348665Z"
    },
    "id": "L3V8sLTc60Vn",
    "papermill": {
     "duration": 0.015772,
     "end_time": "2025-03-24T04:13:57.351145",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.335373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12aa8466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.370020Z",
     "iopub.status.busy": "2025-03-24T04:13:57.369805Z",
     "iopub.status.idle": "2025-03-24T04:13:57.373012Z",
     "shell.execute_reply": "2025-03-24T04:13:57.372062Z"
    },
    "id": "waG3DA9J65ld",
    "papermill": {
     "duration": 0.014473,
     "end_time": "2025-03-24T04:13:57.374557",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.360084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# real_batch = next(iter(dataloader))\n",
    "# plt.figure(figsize=(8,8))\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.imshow(np.transpose(vutils.make_grid(real_batch.to(device)[:8], padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df412ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.393512Z",
     "iopub.status.busy": "2025-03-24T04:13:57.393285Z",
     "iopub.status.idle": "2025-03-24T04:13:57.397364Z",
     "shell.execute_reply": "2025-03-24T04:13:57.396621Z"
    },
    "id": "CYPfLt0n68eZ",
    "papermill": {
     "duration": 0.016029,
     "end_time": "2025-03-24T04:13:57.399275",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.383246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "workers = 4\n",
    "batch_size = 512\n",
    "nc = 3  # Number of channels (RGB)\n",
    "nz = 100  # Size of latent vector\n",
    "ngf = 64  # Size of feature maps in generator\n",
    "ndf = 64  # Size of feature maps in discriminator\n",
    "num_epochs = 5\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ngpu = 1  # Number of GPUs\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27df65c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.418283Z",
     "iopub.status.busy": "2025-03-24T04:13:57.418034Z",
     "iopub.status.idle": "2025-03-24T04:13:57.422723Z",
     "shell.execute_reply": "2025-03-24T04:13:57.421823Z"
    },
    "id": "GVGmXz5K75Bx",
    "outputId": "7c834898-c7a8-4495-869e-ccdfe86d40ce",
    "papermill": {
     "duration": 0.016765,
     "end_time": "2025-03-24T04:13:57.424462",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.407697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bd0d55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.443474Z",
     "iopub.status.busy": "2025-03-24T04:13:57.443244Z",
     "iopub.status.idle": "2025-03-24T04:13:57.451090Z",
     "shell.execute_reply": "2025-03-24T04:13:57.449961Z"
    },
    "id": "phM2G5R2IBF5",
    "outputId": "02579e3b-14c1-4cf5-e8dc-f450d04e57c4",
    "papermill": {
     "duration": 0.019323,
     "end_time": "2025-03-24T04:13:57.452533",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.433210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Hash: hr*_uzy-nje(d=_ph(nd/4l6/t>)#8u=6k(o#689#)b7_ej_(zwox(#c1lmx95$/\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import string\n",
    "import hmac\n",
    "import random\n",
    "\n",
    "# Define the custom character set (0-9, a-z, and special characters)\n",
    "CUSTOM_CHARSET = string.digits + string.ascii_lowercase + \"!@#$%^&*()-_=+<>?/\"\n",
    "\n",
    "def generate_video_key(input_string: str, salt: bytes = None, length: int = 64) -> str:\n",
    "    \"\"\"\n",
    "    Generates a 64-character hash using SHA3-512 and maps it to a custom character set.\n",
    "\n",
    "    - Input: Any string (e.g., user secret, video metadata)\n",
    "    - Output: A unique, reproducible 64-character hash using digits, a-z, and special characters.\n",
    "    \"\"\"\n",
    "    if salt is None:\n",
    "        salt = user_secret.encode()[:8] + b'pranay_pro'  # Use a fixed salt for reproducibility\n",
    "\n",
    "    # Step 1: Generate SHA3-512 hash\n",
    "    raw_hash = hmac.new(salt, input_string.encode(), hashlib.sha3_512).hexdigest()  # 128 hex chars\n",
    "\n",
    "    # Step 2: Convert hex to integer\n",
    "    hash_int = int(raw_hash, 16)\n",
    "    hash_int = ((hash_int >> 3) | (hash_int << 5)) % (2**512)  # Bitwise shuffle\n",
    "\n",
    "    # Step 3: Map hash to the custom character set\n",
    "    custom_hash = []\n",
    "    char_count = len(CUSTOM_CHARSET)\n",
    "\n",
    "    for _ in range(length):  # Ensure exactly `length` characters\n",
    "        custom_hash.append(CUSTOM_CHARSET[hash_int % char_count])\n",
    "        hash_int //= char_count  # Reduce hash_int to get new index\n",
    "\n",
    "    return \"\".join(custom_hash)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ”¹ Example Usage\n",
    "# ------------------------------\n",
    "user_secret = \"my_secret_key\"\n",
    "video_metadata = \"fps:30-resolution:1920x1080-duration:120\"\n",
    "\n",
    "custom_hash = generate_video_key(user_secret + video_metadata)\n",
    "print(\"Custom Hash:\", custom_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e9f35b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.471811Z",
     "iopub.status.busy": "2025-03-24T04:13:57.471587Z",
     "iopub.status.idle": "2025-03-24T04:13:57.475327Z",
     "shell.execute_reply": "2025-03-24T04:13:57.474467Z"
    },
    "id": "YuB-ytwIMiQK",
    "papermill": {
     "duration": 0.015709,
     "end_time": "2025-03-24T04:13:57.477038",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.461329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # for chatgpt crazy logic prime ka\n",
    "# import hashlib\n",
    "# import hmac\n",
    "\n",
    "# # Large prime numbers for obfuscation\n",
    "# PRIME_1 = 15485863  # Large prime\n",
    "# PRIME_2 = 32452843  # Another large prime\n",
    "\n",
    "# def generate_frame_key(global_video_key: str, frame_index: int) -> str:\n",
    "#     \"\"\"\n",
    "#     Derives a unique Frame-Specific Key (FSK) using:\n",
    "#       - HMAC-SHA3-512 for cryptographic security\n",
    "#       - Bitwise XOR, Modulo, and Shift operations for additional obfuscation\n",
    "#     \"\"\"\n",
    "#     salt = b'strong_frame_salt'  # Secure salt\n",
    "\n",
    "#     # Mathematical transformation for additional security\n",
    "#     transformed_index = ((frame_index ^ PRIME_1) * PRIME_2) % (2**32)\n",
    "#     shifted_index = (transformed_index << 3) | (transformed_index >> 5)  # Left and right shift\n",
    "\n",
    "#     # Combine with global_video_key in a non-linear way\n",
    "#     message = f\"{global_video_key[::-1]}|{shifted_index}\".encode('utf-8')  # Reverse global_video_key\n",
    "\n",
    "#     # Generate HMAC-based hash\n",
    "#     frame_key = hmac.new(salt, message, hashlib.sha3_512).hexdigest()\n",
    "\n",
    "#     return frame_key\n",
    "\n",
    "# # ------------------------------\n",
    "# # ðŸ”¹ Example Usage\n",
    "# # ------------------------------\n",
    "# global_video_key = \"abc12345def\"  # Example key\n",
    "# frame_index = 10\n",
    "\n",
    "# strong_frame_key = generate_frame_key(global_video_key, frame_index)\n",
    "# print(\"Stronger Frame Key:\", strong_frame_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f3d42be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.496149Z",
     "iopub.status.busy": "2025-03-24T04:13:57.495937Z",
     "iopub.status.idle": "2025-03-24T04:13:57.501859Z",
     "shell.execute_reply": "2025-03-24T04:13:57.500841Z"
    },
    "id": "qYQkf3guNncm",
    "outputId": "2fe1f2d8-7b99-4dc3-8e40-c3cdf113a76d",
    "papermill": {
     "duration": 0.017584,
     "end_time": "2025-03-24T04:13:57.503460",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.485876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stronger Frame Key: 7ea42ccafc77578e8b7c8123d73f8c0c9278a3172ba6e728b4b1bd8d607d5911eeb46e700d393017ea444ef099e20dd63593d1d8753d6e5fc53d36c6bf5a5285\n"
     ]
    }
   ],
   "source": [
    "# XOR ka logic jho mujhe samjha and sahi laga thoda\n",
    "import hashlib\n",
    "import hmac\n",
    "\n",
    "\n",
    "\n",
    "def generate_frame_key(global_video_key: str, frame_index: int) -> str:\n",
    "    \"\"\"\n",
    "    Derives a unique Frame-Specific Key (FSK) using:\n",
    "      - HMAC-SHA3-512 for cryptographic security\n",
    "      - Bitwise XOR, Modulo, and Shift operations for additional obfuscation\n",
    "    \"\"\"\n",
    "    salt = b'Pranay_pro'  # Secure salt\n",
    "\n",
    "    # Mathematical transformation for additional security\n",
    "    transformed_index = frame_index ^ 0xA5A5A5A5  # Bitwise XOR with fixed mask\n",
    "\n",
    "\n",
    "    shifted_index = (transformed_index << 3) | (transformed_index >> 5)  # Left and right shift\n",
    "\n",
    "    # Combine with global_video_key in a non-linear way\n",
    "    message = f\"{global_video_key}^{shifted_index}\".encode('utf-8')  # Reverse global_video_key\n",
    "\n",
    "    # Generate HMAC-based hash\n",
    "    frame_key = hmac.new(salt, message, hashlib.sha3_512).hexdigest()\n",
    "\n",
    "    return frame_key\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ”¹ Example Usage\n",
    "# ------------------------------\n",
    "global_video_key = \"abc12345def\"  # Example key\n",
    "frame_index = 10\n",
    "\n",
    "strong_frame_key = generate_frame_key(global_video_key, frame_index)\n",
    "print(\"Stronger Frame Key:\", strong_frame_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa17b18f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.522766Z",
     "iopub.status.busy": "2025-03-24T04:13:57.522538Z",
     "iopub.status.idle": "2025-03-24T04:13:57.525849Z",
     "shell.execute_reply": "2025-03-24T04:13:57.524857Z"
    },
    "id": "I3IX7TMX76uc",
    "papermill": {
     "duration": 0.014847,
     "end_time": "2025-03-24T04:13:57.527254",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.512407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_video_key(user_secret: str, video_metadata: dict) -> str:\n",
    "#     \"\"\"\n",
    "#     Generates a Global Video Key (GVK) based on user secret and video metadata.\n",
    "#     \"\"\"\n",
    "#     meta_string = f\"{video_metadata['fps']}-{video_metadata['resolution']}-{video_metadata['duration']}\"\n",
    "#     combined = f\"{user_secret}+{meta_string}\"\n",
    "#     video_key = hashlib.sha3_256(combined.encode()).hexdigest()\n",
    "#     return video_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a80355c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.546421Z",
     "iopub.status.busy": "2025-03-24T04:13:57.546193Z",
     "iopub.status.idle": "2025-03-24T04:13:57.549687Z",
     "shell.execute_reply": "2025-03-24T04:13:57.548599Z"
    },
    "id": "D-ipQDnuL3cz",
    "papermill": {
     "duration": 0.014939,
     "end_time": "2025-03-24T04:13:57.551102",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.536163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_frame_key(global_video_key: str, frame_index: int) -> str:\n",
    "#     \"\"\"\n",
    "#     Derives a unique Frame-Specific Key (FSK) using the GVK and frame index.\n",
    "#     \"\"\"\n",
    "#     combined = f\"{global_video_key}-{frame_index}\"\n",
    "#     frame_key = hashlib.sha3_256(combined.encode()).hexdigest()\n",
    "#     return frame_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f6bd25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.569918Z",
     "iopub.status.busy": "2025-03-24T04:13:57.569702Z",
     "iopub.status.idle": "2025-03-24T04:13:57.574976Z",
     "shell.execute_reply": "2025-03-24T04:13:57.573980Z"
    },
    "id": "el3RE90JN9TF",
    "outputId": "e6217d87-8fc7-4803-ef4d-a92355be7426",
    "papermill": {
     "duration": 0.016537,
     "end_time": "2025-03-24T04:13:57.576469",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.559932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Secrets: ['ucwomduiuvcz', 'egosgwzfqbaf', 'znerexoymstd', 'qmtgcvguhnbu', 'lcxoefufnhgp']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_strings(num_strings=50, length=12):\n",
    "    \"\"\"\n",
    "    Generates a list of `num_strings` random English-like words.\n",
    "    Each word is `length` characters long and made of random English letters.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for _ in range(num_strings):\n",
    "        word = \"\".join(random.choices(string.ascii_lowercase, k=length))  # Random English-like word\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "# Generate 50 random user secrets\n",
    "user_secrets = generate_random_strings(num_strings=50, length=12)\n",
    "\n",
    "# Print example user secrets\n",
    "print(\"User Secrets:\", user_secrets[:5])  # Print first 5 for preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32f6a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:13:57.595869Z",
     "iopub.status.busy": "2025-03-24T04:13:57.595655Z",
     "iopub.status.idle": "2025-03-24T04:14:24.018649Z",
     "shell.execute_reply": "2025-03-24T04:14:24.016775Z"
    },
    "id": "8idUP-lDI7fI",
    "outputId": "75f23f91-e509-4713-b6c2-1dca6b44cb06",
    "papermill": {
     "duration": 26.435758,
     "end_time": "2025-03-24T04:14:24.021079",
     "exception": false,
     "start_time": "2025-03-24T04:13:57.585321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loaded!\n",
      "Images Shape: torch.Size([512, 3, 128, 128])\n",
      "First User Secret: ntevlhokudik\n",
      "First Frame Index: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(97)\n",
      "First GVK: _@?y^o3l@* ...\n",
      "First FSK: 6cab59776d ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import hmac\n",
    "import hashlib\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class WatermarkedImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted([os.path.join(root, f) for f in os.listdir(root) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
    "\n",
    "        # Generate 50 random user secrets for videos\n",
    "        self.user_secrets = generate_random_strings(num_strings=50, length=12)\n",
    "\n",
    "        # Randomly assign each image to a user secret and frame index\n",
    "        self.image_metadata = []\n",
    "        for img_path in self.image_paths:\n",
    "            user_secret = random.choice(self.user_secrets)  # Random user secret\n",
    "            frame_index = random.randint(1, 100)  # Random frame index\n",
    "            video_metadata = \"fps:30-resolution:1920x1080-duration:120\"\n",
    "            global_video_key = generate_video_key(user_secret+video_metadata)\n",
    "            frame_key = generate_frame_key(global_video_key, frame_index)\n",
    "            self.image_metadata.append((user_secret, frame_index, global_video_key, frame_key))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        user_secret, frame_index, global_video_key, frame_key = self.image_metadata[idx]\n",
    "\n",
    "        return image, user_secret, frame_index, global_video_key, frame_key\n",
    "\n",
    "# ðŸ”¹ Step 5: Define Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ðŸ”¹ Step 6: Create DataLoader for Training GAN\n",
    "dataroot = \"/kaggle/input/flickrfaceshq-dataset-ffhq\"  # Update this with your dataset path\n",
    "dataset = WatermarkedImageDataset(root=dataroot, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=4)\n",
    "\n",
    "# ðŸ”¹ Test DataLoader\n",
    "for batch in dataloader:\n",
    "    images, user_secrets, frame_indices, global_video_keys, frame_keys = batch\n",
    "    print(\"Batch Loaded!\")\n",
    "    print(\"Images Shape:\", images.shape)\n",
    "    print(\"First User Secret:\", user_secrets[0])\n",
    "    print(\"First Frame Index:\", frame_indices[0])\n",
    "    print(\"First GVK:\", global_video_keys[0][:10], \"...\")  # Print first 10 characters\n",
    "    print(\"First FSK:\", frame_keys[0][:10], \"...\")\n",
    "    break  # Exit after first batch test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d8a8bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.041944Z",
     "iopub.status.busy": "2025-03-24T04:14:24.041637Z",
     "iopub.status.idle": "2025-03-24T04:14:24.045683Z",
     "shell.execute_reply": "2025-03-24T04:14:24.044791Z"
    },
    "id": "FuEs8SR4L-tU",
    "papermill": {
     "duration": 0.016591,
     "end_time": "2025-03-24T04:14:24.047303",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.030712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def hash_to_noise_vector(frame_key: str, latent_dim: int = 10) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Converts a frame-specific hash into a noise vector.\n",
    "#     The latent_dim here corresponds to the watermark dimension.\n",
    "#     \"\"\"\n",
    "#     hash_bytes = bytes.fromhex(frame_key)\n",
    "#     hash_ints = list(hash_bytes)\n",
    "#     # Ensure we have enough integers; repeat if needed\n",
    "#     if len(hash_ints) < latent_dim:\n",
    "#         hash_ints = (hash_ints * (latent_dim // len(hash_ints) + 1))[:latent_dim]\n",
    "#     else:\n",
    "#         hash_ints = hash_ints[:latent_dim]\n",
    "#     # Normalize values to [-1, 1]\n",
    "#     noise_array = np.array(hash_ints) / 128.0 - 1.0\n",
    "#     return torch.tensor(noise_array, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb509c1",
   "metadata": {
    "id": "k6mwY22NWvQb",
    "papermill": {
     "duration": 0.008955,
     "end_time": "2025-03-24T04:14:24.065374",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.056419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd8a7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.085012Z",
     "iopub.status.busy": "2025-03-24T04:14:24.084782Z",
     "iopub.status.idle": "2025-03-24T04:14:24.090529Z",
     "shell.execute_reply": "2025-03-24T04:14:24.089551Z"
    },
    "id": "xFjilMhiSS8Y",
    "papermill": {
     "duration": 0.017864,
     "end_time": "2025-03-24T04:14:24.092252",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.074388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def hash_to_noise_vector(frame_key: str, latent_dim: int = 10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a frame-specific hash into a noise vector.\n",
    "    - Uses dataset hash keys in training & testing (no randomness).\n",
    "    - Latent_dim corresponds to the watermark dimension.\n",
    "    \"\"\"\n",
    "    hash_bytes = bytes.fromhex(frame_key)\n",
    "    hash_ints = list(hash_bytes)\n",
    "\n",
    "    # Ensure we have enough integers; repeat if needed\n",
    "    if len(hash_ints) < latent_dim:\n",
    "        hash_ints = (hash_ints * (latent_dim // len(hash_ints) + 1))[:latent_dim]\n",
    "    else:\n",
    "        hash_ints = hash_ints[:latent_dim]\n",
    "\n",
    "    # Normalize values to [-1, 1]\n",
    "    noise_array = np.array(hash_ints) / 128.0 - 1.0\n",
    "    return torch.tensor(noise_array, dtype=torch.float32)\n",
    "\n",
    "# def get_watermark_tensor(batch_size, watermark_dim, global_video_keys, frame_indices):\n",
    "#     \"\"\"\n",
    "#     Returns a watermark tensor:\n",
    "#       âœ… Uses dataset hash-based noise for both training & testing.\n",
    "#       âœ… No random noise.\n",
    "#       âœ… Fetches noise from dataset-provided keys.\n",
    "\n",
    "#     global_video_keys: List of video keys (batch size length).\n",
    "#     frame_indices: List of frame indices (batch size length).\n",
    "#     \"\"\"\n",
    "#     watermarks = []\n",
    "#     for gvk, idx in zip(global_video_keys, frame_indices):\n",
    "#         frame_key = generate_frame_key(gvk, idx.item())  # Ensure idx is int\n",
    "#         watermark = hash_to_noise_vector(frame_key, latent_dim=watermark_dim)\n",
    "#         watermarks.append(watermark)\n",
    "\n",
    "#     watermarks = torch.stack(watermarks).unsqueeze(-1).unsqueeze(-1)  # Shape: [B, watermark_dim, 1, 1]\n",
    "#     return watermarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a274aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.113276Z",
     "iopub.status.busy": "2025-03-24T04:14:24.113039Z",
     "iopub.status.idle": "2025-03-24T04:14:24.116539Z",
     "shell.execute_reply": "2025-03-24T04:14:24.115665Z"
    },
    "id": "cvst6ZvkMKcG",
    "papermill": {
     "duration": 0.016277,
     "end_time": "2025-03-24T04:14:24.118108",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.101831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_watermark_tensor(batch_size, watermark_dim, training=True, global_video_key=None, frame_indices=None):\n",
    "#     \"\"\"\n",
    "#     Returns a watermark tensor:\n",
    "#       - In training mode, returns random noise.\n",
    "#       - In testing mode, uses hash functions to generate a unique noise vector for each frame.\n",
    "#     \"\"\"\n",
    "#     if training:\n",
    "#         return torch.randn(batch_size, watermark_dim, 1, 1)\n",
    "#     else:\n",
    "#         watermarks = []\n",
    "#         for idx in frame_indices:\n",
    "#             frame_key = generate_frame_key(global_video_key, idx)\n",
    "#             watermark = hash_to_noise_vector(frame_key, latent_dim=watermark_dim)\n",
    "#             watermarks.append(watermark)\n",
    "#         watermarks = torch.stack(watermarks).unsqueeze(-1).unsqueeze(-1)  # Shape: [B, watermark_dim, 1, 1]\n",
    "#         return watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "924f935e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.138087Z",
     "iopub.status.busy": "2025-03-24T04:14:24.137860Z",
     "iopub.status.idle": "2025-03-24T04:14:24.142646Z",
     "shell.execute_reply": "2025-03-24T04:14:24.141167Z"
    },
    "id": "ydRU4RaPMLJ9",
    "papermill": {
     "duration": 0.016118,
     "end_time": "2025-03-24T04:14:24.143505",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.127387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class PixelNoise(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Injects pixel-level noise into feature maps.\n",
    "#     - In training mode, uses random noise.\n",
    "#     - In testing mode (when use_hash_noise is True and hash parameters are provided),\n",
    "#       it uses deterministic hash-based noise.\n",
    "\n",
    "#     Note: The original \"num_pixels\" parameter is kept for compatibility, but here we override\n",
    "#     it by using the input's channel dimension.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_pixels, noise_intensity):\n",
    "#         super(PixelNoise, self).__init__()\n",
    "#         self.num_pixels = num_pixels           # Original intended dimension (ignored in forward)\n",
    "#         self.noise_intensity = noise_intensity\n",
    "#         # Flags/attributes to control hash-based noise:\n",
    "#         self.use_hash_noise = False            # Default: random noise for training\n",
    "#         self.global_video_key = None\n",
    "#         self.frame_indices = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, channels, H, W = x.shape\n",
    "#         # Use the input's channel dimension as the noise dimension\n",
    "#         noise_dim = channels\n",
    "\n",
    "#         if self.use_hash_noise and self.global_video_key is not None and self.frame_indices is not None:\n",
    "#             # Generate deterministic noise using the hash-based functions\n",
    "#             noise = get_watermark_tensor(batch_size, noise_dim, training=False,\n",
    "#                                          global_video_key=self.global_video_key, frame_indices=self.frame_indices)\n",
    "#             # Expand to match the spatial dimensions of x\n",
    "#             noise = noise.expand(batch_size, noise_dim, H, W)\n",
    "#         else:\n",
    "#             # Generate random noise (for training)\n",
    "#             noise = torch.randn(batch_size, noise_dim, H, W, device=x.device)\n",
    "#         return x + noise * self.noise_intensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "298efee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.163834Z",
     "iopub.status.busy": "2025-03-24T04:14:24.163499Z",
     "iopub.status.idle": "2025-03-24T04:14:24.173712Z",
     "shell.execute_reply": "2025-03-24T04:14:24.172462Z"
    },
    "id": "3XHbKGniTO_S",
    "papermill": {
     "duration": 0.022553,
     "end_time": "2025-03-24T04:14:24.175194",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.152641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PixelNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects pixel-level noise into feature maps.\n",
    "    - Uses `frame_keys` from DataLoader (NO recomputation).\n",
    "    - Learns optimal:\n",
    "        âœ… Watermark intensity (invisibility factor).\n",
    "        âœ… X, Y coordinates for embedding the watermark.\n",
    "        âœ… Patch size (relative to image size).\n",
    "    \"\"\"\n",
    "    def __init__(self, init_intensity=0.05, init_patch_ratio=1/3):\n",
    "        super(PixelNoise, self).__init__()\n",
    "\n",
    "        # Learnable parameters (optimized during training)\n",
    "        self.noise_intensity = nn.Parameter(torch.tensor(init_intensity, dtype=torch.float32))  # Intensity\n",
    "        self.patch_x = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))  # X coordinate (normalized 0-1)\n",
    "        self.patch_y = nn.Parameter(torch.tensor(0.5, dtype=torch.float32))  # Y coordinate (normalized 0-1)\n",
    "        self.patch_size_ratio = nn.Parameter(torch.tensor(init_patch_ratio, dtype=torch.float32))  # Patch size ratio\n",
    "\n",
    "    def forward(self, x, frame_keys):\n",
    "        \"\"\"\n",
    "        x: Batch of images (B, C, H, W)\n",
    "        frame_keys: Batch of frame-specific keys from DataLoader\n",
    "        \"\"\"\n",
    "        batch_size, channels, H, W = x.shape\n",
    "        noise_dim = channels  # Noise dimension matches input channels\n",
    "\n",
    "        # Convert frame keys into noise vectors\n",
    "        noise_list = [hash_to_noise_vector(frame_keys[i % batch_size], latent_dim=noise_dim) for i in range(batch_size)]\n",
    "        noise = torch.stack(noise_list).unsqueeze(-1).unsqueeze(-1).to(x.device)  # Shape: [B, noise_dim, 1, 1]\n",
    "        noise = noise.expand(batch_size, noise_dim, H, W)  # Match spatial dimensions\n",
    "\n",
    "        # Convert learnable X, Y positions into actual pixel locations\n",
    "        patch_H = int(H * self.patch_size_ratio.item())\n",
    "        patch_W = int(W * self.patch_size_ratio.item())\n",
    "        x_offset = int(self.patch_x.item() * (H - patch_H))\n",
    "        y_offset = int(self.patch_y.item() * (W - patch_W))\n",
    "\n",
    "        # Embed noise ONLY in the learned patch region\n",
    "        # âœ… Create a copy of x to avoid inplace operation:\n",
    "        x_noisy = x.clone()\n",
    "\n",
    "        # Embed noise ONLY in the learned patch region\n",
    "        x_noisy[:, :, x_offset:x_offset + patch_H, y_offset:y_offset + patch_W] += (\n",
    "            noise[:, :, x_offset:x_offset + patch_H, y_offset:y_offset + patch_W] * self.noise_intensity\n",
    "        )\n",
    "\n",
    "        return x_noisy # âœ… Return the copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5527e6ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.195691Z",
     "iopub.status.busy": "2025-03-24T04:14:24.195457Z",
     "iopub.status.idle": "2025-03-24T04:14:24.200355Z",
     "shell.execute_reply": "2025-03-24T04:14:24.199156Z"
    },
    "id": "4QBh_CDRjPQn",
    "papermill": {
     "duration": 0.016928,
     "end_time": "2025-03-24T04:14:24.201585",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.184657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.2)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.2)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5266201f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.221362Z",
     "iopub.status.busy": "2025-03-24T04:14:24.221110Z",
     "iopub.status.idle": "2025-03-24T04:14:24.225862Z",
     "shell.execute_reply": "2025-03-24T04:14:24.224666Z"
    },
    "id": "Svvs2qcPYgkV",
    "papermill": {
     "duration": 0.016438,
     "end_time": "2025-03-24T04:14:24.227065",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.210627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, ngpu, nz, ngf, nc):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         # The generator structure is left untouched.\n",
    "#         self.main = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 16),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             PixelNoise(num_pixels=50, noise_intensity=0.05),\n",
    "\n",
    "#             nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             PixelNoise(num_pixels=40, noise_intensity=0.04),\n",
    "\n",
    "#             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 4),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             PixelNoise(num_pixels=30, noise_intensity=0.03),\n",
    "\n",
    "#             nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf * 2),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             PixelNoise(num_pixels=20, noise_intensity=0.02),\n",
    "\n",
    "#             nn.ConvTranspose2d(ngf, ngf, 3, 1, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ngf),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de8a5d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.246985Z",
     "iopub.status.busy": "2025-03-24T04:14:24.246759Z",
     "iopub.status.idle": "2025-03-24T04:14:24.255609Z",
     "shell.execute_reply": "2025-03-24T04:14:24.254344Z"
    },
    "id": "0ROECQDRbH9L",
    "papermill": {
     "duration": 0.020187,
     "end_time": "2025-03-24T04:14:24.256612",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.236425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        # Define PixelNoise layers separately so they can receive frame_keys\n",
    "        self.pixel_noise_1 = PixelNoise(init_intensity=0.05, init_patch_ratio=1/3)\n",
    "\n",
    "\n",
    "        # Define main generator architecture\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, ngf, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, frame_keys):\n",
    "        \"\"\"\n",
    "        input: Random noise vector (B, nz, 1, 1)\n",
    "        frame_keys: List of frame-specific hash keys from DataLoader\n",
    "        \"\"\"\n",
    "        x = self.main(input)\n",
    "\n",
    "        # Apply PixelNoise layers (Pass frame_keys correctly)\n",
    "        x = self.pixel_noise_1(x, frame_keys)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7c39bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.276265Z",
     "iopub.status.busy": "2025-03-24T04:14:24.276045Z",
     "iopub.status.idle": "2025-03-24T04:14:24.282786Z",
     "shell.execute_reply": "2025-03-24T04:14:24.281745Z"
    },
    "id": "eg85TfiXde3L",
    "papermill": {
     "duration": 0.018541,
     "end_time": "2025-03-24T04:14:24.284314",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.265773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, nc=3, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input: Real or Fake images (Batch, Channels, H, W)\n",
    "        \"\"\"\n",
    "        return self.main(input)  # âœ… Processes batch images properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5851418c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T04:14:24.304375Z",
     "iopub.status.busy": "2025-03-24T04:14:24.304153Z",
     "iopub.status.idle": "2025-03-24T06:01:33.624139Z",
     "shell.execute_reply": "2025-03-24T06:01:33.622047Z"
    },
    "id": "8kMHTCIedgP1",
    "outputId": "15e3cef4-59a2-4ba6-be9d-1073520c2919",
    "papermill": {
     "duration": 6429.332374,
     "end_time": "2025-03-24T06:01:33.626174",
     "exception": false,
     "start_time": "2025-03-24T04:14:24.293800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (pixel_noise_1): PixelNoise()\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (12): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (15): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (18): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][0/102]\tLoss_D: 15.4965\tLoss_G: 6.1761\tD(x): 0.5353\tD(G(z)): 0.4688 / 0.4340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][50/102]\tLoss_D: 5.8452\tLoss_G: 8.7593\tD(x): 0.6860\tD(G(z)): 0.3397 / 0.2284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][100/102]\tLoss_D: 2.0105\tLoss_G: 11.1185\tD(x): 0.8124\tD(G(z)): 0.2176 / 0.1067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5][0/102]\tLoss_D: 2.0330\tLoss_G: 11.4936\tD(x): 0.8093\tD(G(z)): 0.2086 / 0.0998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5][50/102]\tLoss_D: 0.7319\tLoss_G: 15.0619\tD(x): 0.8665\tD(G(z)): 0.0434 / 0.0987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5][100/102]\tLoss_D: 0.6564\tLoss_G: 10.0774\tD(x): 0.9119\tD(G(z)): 0.1365 / 0.0473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/5][0/102]\tLoss_D: 0.4956\tLoss_G: 10.2855\tD(x): 0.9262\tD(G(z)): 0.1033 / 0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/5][50/102]\tLoss_D: 0.4056\tLoss_G: 8.7437\tD(x): 0.9338\tD(G(z)): 0.0866 / 0.0186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/5][100/102]\tLoss_D: 0.4573\tLoss_G: 8.3710\tD(x): 0.9255\tD(G(z)): 0.0980 / 0.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/5][0/102]\tLoss_D: 0.3963\tLoss_G: 9.0935\tD(x): 0.9295\tD(G(z)): 0.0811 / 0.0068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/5][50/102]\tLoss_D: 0.2529\tLoss_G: 8.3515\tD(x): 0.9519\tD(G(z)): 0.0667 / 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/5][100/102]\tLoss_D: 0.1955\tLoss_G: 8.0136\tD(x): 0.9572\tD(G(z)): 0.0468 / 0.0090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/5][0/102]\tLoss_D: 0.2120\tLoss_G: 7.9207\tD(x): 0.9540\tD(G(z)): 0.0548 / 0.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/5][50/102]\tLoss_D: 0.2237\tLoss_G: 7.5465\tD(x): 0.9505\tD(G(z)): 0.0564 / 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/5][100/102]\tLoss_D: 0.1520\tLoss_G: 8.4152\tD(x): 0.9749\tD(G(z)): 0.0646 / 0.0213\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# âœ… Create the generator\n",
    "netG = Generator(ngpu, nz, ngf, nc).to(device)\n",
    "\n",
    "# âœ… Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# âœ… Apply the weights_init function to randomly initialize all weights\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# âœ… Print the model\n",
    "print(netG)\n",
    "\n",
    "\n",
    "# âœ… Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# âœ… Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# âœ… Apply the weights_init function\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# âœ… Print the model\n",
    "print(netD)\n",
    "\n",
    "\n",
    "# âœ… Define Loss Function and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# âœ… Create fixed noise for tracking progress\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "# âœ… Initialize tracking lists\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "\n",
    "# âœ… Start Training Loop\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update Discriminator\n",
    "        ############################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # âœ… Load real images & frame keys\n",
    "        real_cpu = data[0].to(device)  # Images from dataset\n",
    "        frame_keys = data[4]  # Extract frame_keys from DataLoader\n",
    "\n",
    "        # âœ… Ensure proper shape for batch processing\n",
    "        if real_cpu.ndim == 3:\n",
    "            real_cpu = real_cpu.unsqueeze(0)\n",
    "\n",
    "        b_size = real_cpu.size(0)\n",
    "\n",
    "        # âœ… Forward pass real batch through D\n",
    "        output_real = netD(real_cpu)\n",
    "        target_real = torch.full_like(output_real, real_label, device=device)\n",
    "        errD_real = criterion(output_real, target_real)\n",
    "        errD_real.backward()\n",
    "        D_x = output_real.mean().item()\n",
    "\n",
    "        # âœ… Train with fake images\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise, frame_keys)  # âœ… Pass frame_keys into Generator\n",
    "\n",
    "        output_fake = netD(fake.detach())\n",
    "        target_fake = torch.full_like(output_fake, fake_label, device=device)\n",
    "        errD_fake = criterion(output_fake, target_fake)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output_fake.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update Generator\n",
    "        ############################\n",
    "        netG.zero_grad()\n",
    "        output_fake_forG = netD(fake)\n",
    "        target_forG = torch.full_like(output_fake_forG, real_label, device=device)\n",
    "        errG = criterion(output_fake_forG, target_forG)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output_fake_forG.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # âœ… Print training stats every 50 mini-batches\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # âœ… Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # âœ… Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_images = netG(fixed_noise, frame_keys[:64]).detach().cpu()  # âœ… Pass frame_keys\n",
    "            img_list.append(vutils.make_grid(fake_images, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0dfb029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.650768Z",
     "iopub.status.busy": "2025-03-24T06:01:33.650444Z",
     "iopub.status.idle": "2025-03-24T06:01:33.655991Z",
     "shell.execute_reply": "2025-03-24T06:01:33.654586Z"
    },
    "id": "QbatmuwOZzeA",
    "papermill": {
     "duration": 0.019046,
     "end_time": "2025-03-24T06:01:33.657024",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.637978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, ngpu, nc=3, ndf=64):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.main = nn.Sequential(\n",
    "#             nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 2),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 4),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "#             nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2046a249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.680883Z",
     "iopub.status.busy": "2025-03-24T06:01:33.680643Z",
     "iopub.status.idle": "2025-03-24T06:01:33.685000Z",
     "shell.execute_reply": "2025-03-24T06:01:33.683666Z"
    },
    "id": "DR_TgSZAZzbu",
    "papermill": {
     "duration": 0.019034,
     "end_time": "2025-03-24T06:01:33.687196",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.668162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create the Discriminator\n",
    "# netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# # Handle multi-gpu if desired\n",
    "# if (device.type == 'cuda') and (ngpu > 1):\n",
    "#     netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# # Apply the weights_init function to randomly initialize all weights\n",
    "# #  to mean=0, stdev=0.2.\n",
    "# netD.apply(weights_init)\n",
    "\n",
    "# # Print the model\n",
    "# print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b1b6208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.718532Z",
     "iopub.status.busy": "2025-03-24T06:01:33.718300Z",
     "iopub.status.idle": "2025-03-24T06:01:33.722130Z",
     "shell.execute_reply": "2025-03-24T06:01:33.720806Z"
    },
    "id": "NN2myu3LZ5iS",
    "papermill": {
     "duration": 0.022862,
     "end_time": "2025-03-24T06:01:33.723731",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.700869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# lr = 0.0002\n",
    "# beta1 = 0.5\n",
    "\n",
    "# optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "# optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "# real_label = 1\n",
    "# fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae4ecc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.759898Z",
     "iopub.status.busy": "2025-03-24T06:01:33.759652Z",
     "iopub.status.idle": "2025-03-24T06:01:33.765973Z",
     "shell.execute_reply": "2025-03-24T06:01:33.764573Z"
    },
    "id": "ZrkfoHlwZ5f2",
    "papermill": {
     "duration": 0.020091,
     "end_time": "2025-03-24T06:01:33.767289",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.747198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_list = []\n",
    "# G_losses = []\n",
    "# D_losses = []\n",
    "# iters = 0\n",
    "\n",
    "# print(\"Starting Training Loop...\")\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, data in enumerate(dataloader, 0):\n",
    "#         ############################\n",
    "#         # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "#         ############################\n",
    "#         netD.zero_grad()\n",
    "#         real_cpu = data[0].to(device)\n",
    "#         # Ensure the input has 4 dimensions (batch, channels, height, width)\n",
    "#         if real_cpu.ndim == 3:\n",
    "#             real_cpu = real_cpu.unsqueeze(0)\n",
    "#         b_size = real_cpu.size(0)\n",
    "\n",
    "#         # Forward pass real batch through D.\n",
    "#         output_real = netD(real_cpu)\n",
    "#         # Create a target tensor with the same shape as output_real:\n",
    "#         target_real = torch.full_like(output_real, real_label, device=device)\n",
    "#         errD_real = criterion(output_real, target_real)\n",
    "#         errD_real.backward()\n",
    "#         D_x = output_real.mean().item()\n",
    "\n",
    "#         ## Train with all-fake batch\n",
    "#         noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "#         fake = netG(noise)\n",
    "#         output_fake = netD(fake.detach())\n",
    "#         target_fake = torch.full_like(output_fake, fake_label, device=device)\n",
    "#         errD_fake = criterion(output_fake, target_fake)\n",
    "#         errD_fake.backward()\n",
    "#         D_G_z1 = output_fake.mean().item()\n",
    "#         errD = errD_real + errD_fake\n",
    "#         optimizerD.step()\n",
    "\n",
    "#         ############################\n",
    "#         # (2) Update G network: maximize log(D(G(z)))\n",
    "#         ############################\n",
    "#         netG.zero_grad()\n",
    "#         output_fake_forG = netD(fake)\n",
    "#         target_forG = torch.full_like(output_fake_forG, real_label, device=device)\n",
    "#         errG = criterion(output_fake_forG, target_forG)\n",
    "#         errG.backward()\n",
    "#         D_G_z2 = output_fake_forG.mean().item()\n",
    "#         optimizerG.step()\n",
    "\n",
    "#         # Print training stats every 50 mini-batches\n",
    "#         if i % 50 == 0:\n",
    "#             print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "#                   % (epoch, num_epochs, i, len(dataloader),\n",
    "#                      errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "#         # Save Losses for plotting later\n",
    "#         G_losses.append(errG.item())\n",
    "#         D_losses.append(errD.item())\n",
    "\n",
    "#         # Check how the generator is doing by saving G's output on fixed_noise\n",
    "#         if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "#             with torch.no_grad():\n",
    "#                 fake_images = netG(fixed_noise).detach().cpu()\n",
    "#             img_list.append(vutils.make_grid(fake_images, padding=2, normalize=True))\n",
    "#         iters += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "104003f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.811539Z",
     "iopub.status.busy": "2025-03-24T06:01:33.811277Z",
     "iopub.status.idle": "2025-03-24T06:01:33.816959Z",
     "shell.execute_reply": "2025-03-24T06:01:33.815709Z"
    },
    "id": "sQX56mfXZ5dO",
    "papermill": {
     "duration": 0.028325,
     "end_time": "2025-03-24T06:01:33.818466",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.790141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.transforms import ToPILImage\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def add_pixel_noise_to_image_and_save(generator, image_path, device, user_secret, video_metadata, frame_index, output_path, noise_intensity=1.7):\n",
    "#     # Load and preprocess the image (normalize to [-1, 1])\n",
    "#     img = Image.open(image_path).convert('RGB')\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),  # Scales to [0,1]\n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Now in [-1,1]\n",
    "#     ])\n",
    "#     image_tensor = transform(img).to(device)  # Shape: (C, H, W), move to device here\n",
    "\n",
    "#     # Set generator to evaluation mode\n",
    "#     generator.eval()\n",
    "\n",
    "#     # Compute the global video key and frame key\n",
    "#     global_video_key = generate_video_key(user_secret, video_metadata)\n",
    "#     frame_key = generate_frame_key(global_video_key, frame_index)\n",
    "\n",
    "#     # Generate hash-based noise vector\n",
    "#     watermark = hash_to_noise_vector(frame_key, latent_dim=nz)  # Use nz for noise dimension\n",
    "#     watermark = watermark.unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)  # Reshape for generator\n",
    "\n",
    "\n",
    "#     # Configure all PixelNoise modules in the generator to use hash-based noise\n",
    "#     for module in generator.modules():\n",
    "#         if isinstance(module, PixelNoise):\n",
    "#             module.use_hash_noise = True\n",
    "#             module.global_video_key = global_video_key\n",
    "#             module.frame_indices = [frame_index]  # For a batch size of 1\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # Generate noise pattern using the generator\n",
    "#         noise_pattern = generator(watermark)\n",
    "\n",
    "#         # Resize the noise pattern to the original image size using interpolation\n",
    "#         noise_pattern = F.interpolate(noise_pattern, size=image_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#         # Apply the noise pattern to the image\n",
    "#         noisy_tensor = image_tensor + noise_pattern.squeeze(0) * noise_intensity\n",
    "\n",
    "#     # Move to CPU, Convert back from [-1, 1] to [0, 1] for image saving\n",
    "#     noisy_tensor = noisy_tensor.cpu()\n",
    "#     noisy_tensor = (noisy_tensor + 1) / 2.0\n",
    "#     noisy_tensor = torch.clamp(noisy_tensor, 0, 1)\n",
    "\n",
    "#     # Convert the tensor to a PIL image\n",
    "#     to_pil = ToPILImage()\n",
    "#     noisy_image = to_pil(noisy_tensor)\n",
    "\n",
    "#     # Save the image to disk with the given output_path (including file extension)\n",
    "#     noisy_image.save(output_path)\n",
    "\n",
    "#     return noisy_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7756b874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.848205Z",
     "iopub.status.busy": "2025-03-24T06:01:33.847962Z",
     "iopub.status.idle": "2025-03-24T06:01:33.853644Z",
     "shell.execute_reply": "2025-03-24T06:01:33.852072Z"
    },
    "id": "aJeU4jsMk2W2",
    "papermill": {
     "duration": 0.020797,
     "end_time": "2025-03-24T06:01:33.854743",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.833946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def add_pixel_noise_to_image_and_save(generator, image_path, device, user_secret, video_metadata, frame_index, output_path, noise_intensity=1.7):\n",
    "#     # Load and preprocess the image (normalize to [-1, 1])\n",
    "#     img = Image.open(image_path).convert('RGB')\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),  # Scales to [0,1]\n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Now in [-1,1]\n",
    "#     ])\n",
    "#     image_tensor = transform(img).to(device)  # Shape: (C, H, W), move to device here\n",
    "\n",
    "#     # Set generator to evaluation mode\n",
    "#     generator.eval()\n",
    "\n",
    "#     # Compute the global video key and frame key\n",
    "#     global_video_key = generate_video_key(user_secret, video_metadata)\n",
    "#     frame_key = generate_frame_key(global_video_key, frame_index)\n",
    "\n",
    "#     # Generate hash-based noise vector\n",
    "#     watermark = hash_to_noise_vector(frame_key, latent_dim=nz)  # Use nz for noise dimension\n",
    "#     watermark = watermark.unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)  # Reshape for generator\n",
    "\n",
    "\n",
    "#     # Configure all PixelNoise modules in the generator to use hash-based noise\n",
    "#     for module in generator.modules():\n",
    "#         if isinstance(module, PixelNoise):\n",
    "#             module.use_hash_noise = True\n",
    "#             module.global_video_key = global_video_key\n",
    "#             module.frame_indices = [frame_index]  # For a batch size of 1\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # Generate noise pattern using the generator\n",
    "#         noise_pattern = generator(watermark)\n",
    "\n",
    "#         # Resize the noise pattern to the original image size using interpolation\n",
    "#         noise_pattern = F.interpolate(noise_pattern, size=image_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#         # Apply the noise pattern to the image\n",
    "#         noisy_tensor = image_tensor + noise_pattern.squeeze(0) * noise_intensity\n",
    "\n",
    "#     # Move to CPU, Convert back from [-1, 1] to [0, 1] for image saving\n",
    "#     noisy_tensor = noisy_tensor.cpu()\n",
    "#     noisy_tensor = (noisy_tensor + 1) / 2.0\n",
    "#     noisy_tensor = torch.clamp(noisy_tensor, 0, 1)\n",
    "\n",
    "#     # Convert the tensor to a PIL image\n",
    "#     to_pil = ToPILImage()\n",
    "#     noisy_image = to_pil(noisy_tensor)\n",
    "\n",
    "#     # Save the image to disk with the given output_path (including file extension)\n",
    "#     noisy_image.save(output_path)\n",
    "\n",
    "#     return noisy_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb288153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.878780Z",
     "iopub.status.busy": "2025-03-24T06:01:33.878566Z",
     "iopub.status.idle": "2025-03-24T06:01:33.882938Z",
     "shell.execute_reply": "2025-03-24T06:01:33.881641Z"
    },
    "id": "d2A7veha2Q-O",
    "papermill": {
     "duration": 0.017712,
     "end_time": "2025-03-24T06:01:33.884001",
     "exception": false,
     "start_time": "2025-03-24T06:01:33.866289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt: now for above function make a call with user_secret = \"my_secret_key\"\n",
    "# video_metadata = {\"fps\": 30, \"resolution\": \"1920x1080\", \"duration\": 120}\n",
    "# frame_index = 10\n",
    "# input_image_path = \"/content/images/00003.png\"\n",
    "# output_image_path = \"output_noisy_image.jpg\"\n",
    "# # Call the function with the sample parameters\n",
    "# noisy_image = add_pixel_noise_to_image_and_save(\n",
    "#     generator=netG,\n",
    "#     image_path=input_image_path,\n",
    "#     device=device,\n",
    "#     user_secret=user_secret,\n",
    "#     video_metadata=video_metadata,\n",
    "#     frame_index=frame_index,\n",
    "#     output_path=output_image_path\n",
    "# )\n",
    "# print(\"Noisy image saved at:\", output_image_path)\n",
    "\n",
    "# user_secret = \"my_secret_key\"\n",
    "# video_metadata = {\"fps\": 30, \"resolution\": \"1920x1080\", \"duration\": 120}\n",
    "# frame_index = 10\n",
    "# input_image_path = \"/content/images/00003.png\"\n",
    "# output_image_path = \"output_noisy_image_0.1.jpg\"\n",
    "\n",
    "\n",
    "\n",
    "# # Call the function with the sample parameters\n",
    "# noisy_image = add_pixel_noise_to_image_and_save(\n",
    "#     generator=netG,\n",
    "#     image_path=input_image_path,\n",
    "#     device=device,\n",
    "#     user_secret=user_secret,\n",
    "#     video_metadata=video_metadata,\n",
    "#     frame_index=frame_index,\n",
    "#     output_path=output_image_path,\n",
    "#     noise_intensity=0.1\n",
    "# )\n",
    "\n",
    "# print(\"Noisy image saved at:\", output_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0452c840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T06:01:33.908337Z",
     "iopub.status.busy": "2025-03-24T06:01:33.908128Z",
     "iopub.status.idle": "2025-03-24T06:01:34.602449Z",
     "shell.execute_reply": "2025-03-24T06:01:34.601102Z"
    },
    "id": "rEE_ouC12qvn",
    "papermill": {
     "duration": 0.709498,
     "end_time": "2025-03-24T06:01:34.604265",
     "exception": true,
     "start_time": "2025-03-24T06:01:33.894767",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change to your image path\u001b[39;00m\n\u001b[1;32m     16\u001b[0m output_path_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermarked_image\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Output image base name\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     20\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     21\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m     22\u001b[0m ])\n\u001b[1;32m     23\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move to device\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/PIL/Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input_image.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# âœ… Define user details & metadata\n",
    "user_secret = \"First_testing\"\n",
    "video_metadata = \"fps:30-resolution:1920x1080-duration:120\"\n",
    "frame_index = 34  # Fixed frame index\n",
    "salt = b'pranay_pro'  # Fixed salt for reproducibility\n",
    "\n",
    "# âœ… Load and preprocess the image\n",
    "image_path = \"input_image.jpg\"  # Change to your image path\n",
    "output_path_base = \"watermarked_image\"  # Output image base name\n",
    "\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "image_tensor = transform(img).to(device)  # Move to device\n",
    "\n",
    "# âœ… Compute global video key and frame key\n",
    "global_video_key = generate_video_key(user_secret, video_metadata)\n",
    "frame_key = generate_frame_key(global_video_key, frame_index)\n",
    "\n",
    "# âœ… Generate hash-based noise vector\n",
    "watermark = hash_to_noise_vector(frame_key, latent_dim=nz)\n",
    "watermark = watermark.unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)  # Reshape for generator\n",
    "\n",
    "# âœ… Configure the generator\n",
    "generator.eval()\n",
    "for module in generator.modules():\n",
    "    if isinstance(module, PixelNoise):\n",
    "        module.use_hash_noise = True\n",
    "        module.global_video_key = global_video_key\n",
    "        module.frame_indices = [frame_index]\n",
    "        learned_intensity = module.noise_intensity.item()  # âœ… Extract learned optimal intensity\n",
    "\n",
    "# âœ… Generate images using the learned optimal intensity & two increased intensities\n",
    "intensities = [learned_intensity, learned_intensity * 2, learned_intensity * 3]  # ðŸ”¥ Show different visibility levels\n",
    "generated_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for intensity in intensities:\n",
    "        noise_pattern = generator(watermark)\n",
    "\n",
    "        # Resize noise pattern to match image size\n",
    "        noise_pattern = F.interpolate(noise_pattern, size=image_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Apply noise to the image\n",
    "        noisy_tensor = image_tensor + noise_pattern.squeeze(0) * intensity\n",
    "\n",
    "        # Convert back from [-1,1] to [0,1]\n",
    "        noisy_tensor = (noisy_tensor + 1) / 2.0\n",
    "        noisy_tensor = torch.clamp(noisy_tensor, 0, 1)\n",
    "\n",
    "        # Convert tensor to PIL image\n",
    "        to_pil = ToPILImage()\n",
    "        noisy_image = to_pil(noisy_tensor.cpu())\n",
    "\n",
    "        # Save & store images\n",
    "        output_path = f\"{output_path_base}_intensity_{round(intensity, 3)}.jpg\"\n",
    "        noisy_image.save(output_path)\n",
    "        generated_images.append(noisy_image)\n",
    "\n",
    "# âœ… Visualize Results\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "# Original Image\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Images with different watermark intensities\n",
    "titles = [f\"Optimal Intensity ({round(intensities[0], 3)})\",\n",
    "          f\"Slightly Visible ({round(intensities[1], 3)})\",\n",
    "          f\"Fully Visible ({round(intensities[2], 3)})\"]\n",
    "\n",
    "for i, (gen_img, title) in enumerate(zip(generated_images, titles)):\n",
    "    axes[i + 1].imshow(gen_img)\n",
    "    axes[i + 1].set_title(title)\n",
    "    axes[i + 1].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc523c",
   "metadata": {
    "id": "4BNswHV2jZJb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 546691,
     "sourceId": 997012,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6535.736751,
   "end_time": "2025-03-24T06:01:36.922782",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-24T04:12:41.186031",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
